---
authors: "T1 - Phoenix"
title: "Spotify Tracks Popularity Prediction"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r Libraries Used, warning=FALSE, include=FALSE}
library(tidyverse)
library(dplyr)
library(corrplot)
library(mapview)
library(tigris)
library(sf)
library(ggplot2)
library(lubridate)
library(ezids)
library(gridExtra)
library(grid)
library(lattice)
library(caret)
```

# 1. Introduction

Music is something everyone loves. The music industry is one of the biggest industries in the world, the estimated value of it is over $50 billion. Then what makes a song popular? Identifying the factors that affect the popularity of song and predicting it can provide us with a better understanding of the music industry and valuable business insights. Our project is focused on building machine learning models to predict the popularity of song on spotify, by answering two smart questions. The first question is "Which features make a song popular?", and we will determine audio features in a song that affect the popularity. The second question is "Can we recommend a certain song to users who like to listen popular songs?" 

This summary paper is organized as follow:
1. Introduction
2. Description of Data
3. Data Pre-Processing
4. EDA
5. Models
6. Conclusion

# 2. Description of Data
As we were looking for songs dataset, we thought of checking if Spotify which is one of the best music app across the market, provides any data. Thus, initially we attempted to retrieve data using Spotify API. But due to time constraint, we were unable to work through the data. Fortunately, we were able to get the Spotify dataset from Kaggle.  
Let us pull the dataset from the file `spotify_dataset.csv` into a dataframe `df_spotify`. As this file is huge in size, we cannot upload it on GitHub. Thus, we are storing the file in a folder called `spotify_dataset` that is located in one folder above the project folder.

```{r Pulling the Data, results='markup'}
df_spotify <- data.frame(read.csv('spotify_dataset.csv'))
str(df_spotify)
```
It has 114000 observations with 21 variables.   
The data entries include the following variables:  
track_id: ID of track generated by Spotify  
artists: artist’s name  
album_name: name of albums  
track_name: name of tracks  
popularity: ranges from 0 to 100  
duration_ms: duration of track in milliseconds; integer typically ranges from 200k to 300k  
explicit: 0 = no explicit content, 1 = explicit content  
danceability: ranges from 0 to 1  
energy: ranges from 0 to 1  
key: all keys on octave encoded as values ranging from 0 to 11, starting with C as 0, C# as 1, etc.  
loudness: float typically ranging from -60 to 0  
mode: indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0  
speechiness: The higher the value the more spoken word the song contains; ranges from 0 to 1  
acousticness: ranges from 0 to 1  
instrumentalness: the number of vocals in a song; ranges from 0 to 1  
liveness: The higher the value, the more likely the song is a live recording; ranges from 0 to 1  
valence: The higher the value, the more positive mood for the song; ranges from 0 to 1  
tempo: float typically ranging from 50 to 150  
time_signature: an estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of "3/4", to "7/4"  
track_genre: genre of track  
speechiness: ranges from 0 to 1  
acousticness: ranges from 0 to 1  

The target variable for us is "Popularity". It has a range from 0 to 100 and type of integer. The popularity is determined by an algorithm and primarily depends on how recently and how many times the track has been played overall. In general, songs that are played often currently will be more popular than songs that were frequently performed in the past.


```{r, results='markup'}
head(df_spotify)
tail(df_spotify)
```

```{r, results='markup'}
summary(df_spotify)
```

First, let's check the percentage of NA's present in each columns of the dataset.

```{r, Checking NAs}
(colMeans(is.na(df_spotify)))*100
```

```{r, results='markup'}
data_corr<-df_spotify %>% select_if(is.numeric)   
data_corr = subset(data_corr, select = -c(X) )

corrplot(cor(data_corr),
         type = "upper",
         col = colorRampPalette(c("white", "deepskyblue", "blue4"))(100),
         is.corr = TRUE,
         order = "hclust",
         tl.col = "black",
         diag = TRUE)
```

We haven’t dropped any observations since there are no NA values. The columns ‘track id’ and ‘X’ were dropped since they did not have any useful information. 

# 3. Data Pre-Processing

In the Data Preprocessing step, We are performing the following operations

1. Converting the column duration_ms into duration_min. 
2. We are replacing all the unwanted symbols in artists column by blank.

```{r}
df_spotify<-df_spotify %>%
 mutate(duration_min = duration_ms/60000)

df_spotify$duration_min <- round(df_spotify$duration_min ,digit=2)
head(df_spotify)
```

```{r, results='markup'}
df_spotify$artists[df_spotify$artists == '['] <- ''
df_spotify$artists[df_spotify$artists == ']']<- ''
df_spotify$artists[df_spotify$artists == "'"]<- ''
head(df_spotify)
```

# 4. EDA

Let us begin our EDA to answer the first question, "Which features make a song popular?".

```{r, results='markup'}
popHist <- ggplot(df_spotify, aes(x=popularity)) + geom_histogram(color="black", fill="steelblue1", alpha=0.9, bins = 101)+
  ggtitle("Histogram of Popularity")
popHist
```
From the histogram of popularity, we can see there are many songs that have popularity of 0, which made 0 as a mode. There are more values from the left-half side than the right-side, which means it's right skewed.

```{r, results='markup'}
# made basic histogram & scatterplot for every other variables that we didn't include to our SMART Q

danceHist <- ggplot(df_spotify, aes(x=danceability)) + geom_histogram(color="black", fill="steelblue1", alpha=0.9, bins = 150)+
  ggtitle("Histogram of Danceability")
danceHist

danceSct<- ggplot(df_spotify, aes(x=popularity, y= danceability)) +
  geom_point(colour="steelblue1",size = 0.1) +
  ggtitle("Scatterplot for Danceability")
danceSct

acHist <- ggplot(df_spotify, aes(x=acousticness)) + geom_histogram(color="black", fill="steelblue1", alpha=0.9, bins = 100)+
  ggtitle("Histogram of Acousticness")
acHist

acSct<- ggplot(df_spotify, aes(x=popularity, y= acousticness)) +
  geom_point(colour="steelblue1",size = 0.1) +
  ggtitle("Scatterplot for Acousticness")
acSct

energyHist <- ggplot(df_spotify, aes(x=energy)) + geom_histogram(color="black", fill="steelblue1", alpha=0.9, bins = 100)+
  ggtitle("Histogram of Energy")
energyHist

energySct<- ggplot(df_spotify, aes(x=popularity, y= energy)) +
  geom_point(colour="steelblue1",size = 0.1) +
  ggtitle("Scatterplot for Energy")
energySct

speechinessHist <- ggplot(df_spotify, aes(x=speechiness)) + geom_histogram(color="black", fill="steelblue1", alpha=0.9, bins = 100)+
  ggtitle("Histogram of speechiness")
speechinessHist

speechinessSct<- ggplot(df_spotify, aes(x=popularity, y= speechiness)) +
  geom_point(colour="steelblue1",size = 0.1) +
  ggtitle("Scatterplot for speechiness")
speechinessSct

instrumentalnessHist <- ggplot(df_spotify, aes(x=instrumentalness)) + geom_histogram(color="black",fill="steelblue1", alpha=0.9, bins = 50)+
  ggtitle("Histogram of instrumentalness")
instrumentalnessHist

instrumentalnessSctr<- ggplot(df_spotify, aes(x=popularity, y= instrumentalness)) +
  geom_point(colour="steelblue1",size = 0.1) +
  ggtitle("Scatterplot for instrumentalness")
instrumentalnessSctr

livenessHist <- ggplot(df_spotify, aes(x=liveness)) + geom_histogram(color="black",fill="steelblue1", alpha=0.9, bins = 100)+
  ggtitle("Histogram of liveness")
livenessHist

livenessSct<- ggplot(df_spotify, aes(x=popularity, y= liveness)) +
  geom_point(colour="steelblue1",size = 0.1) +
  ggtitle("Scatterplot for liveness")
livenessSct

valenceHist <- ggplot(df_spotify, aes(x=valence)) + geom_histogram(color="black",fill="steelblue1", alpha=0.9, bins = 100)+
  ggtitle("Histogram of valence")
valenceHist

valenceSctr<- ggplot(df_spotify, aes(x=popularity, y= valence)) +
  geom_point(colour="steelblue1",size = 0.1) +
  ggtitle("Scatterplot for valence")
valenceSctr

tempoHist <- ggplot(df_spotify, aes(x=tempo)) + geom_histogram(color="black",fill="steelblue1", alpha=0.9, bins = 100)+
  ggtitle("Histogram of tempo")
tempoHist

tempoSctr<- ggplot(df_spotify, aes(x=popularity, y= tempo)) +
  geom_point(colour="steelblue1",size = 0.1) +
  ggtitle("Scatterplot for tempo")
tempoSctr
```
We can see the distribution of variables with histograms.
There are many right-skewed histograms including the histograms of Accousticness, Speechiness, Liveness, and Instrumentalness. However, for Danceability and Energy, the graphs are left-skewed. For tempo, the graph looks bimodal. And it looks like the Valence has an uniform histogram. 
By scatterplots, we can see the distribution of each variables with its popularity.


```{r, results='markup'}
library("ggpubr")
histfig <- ggarrange(danceHist,acHist,energyHist, speechinessHist, instrumentalnessHist,livenessHist, valenceHist,tempoHist, ncol = 3, nrow = 2)
histfig

sctfig <- ggarrange(danceSct, acSct,energySct, speechinessSct, instrumentalnessSctr,livenessSct, valenceSctr,tempoSctr, ncol = 3, nrow = 2)
sctfig
```

Here you can see the correlation plot for the features of a song. As we can see in the plot, a lot of features are correlated to each other. For example, energy and loudness, danceability and valence etc. Coming to target variable popularity, it doesn’t really have a strong relation with any of the other features, nonetheless, there is some relationship there. For instance, instrumentalness, danceability, valence etc. We will try to understand these relations in dept in the slides ahead.

```{r}
data_corr<-df_spotify[!duplicated(df_spotify),]
data_corr = subset(data_corr, select = -c(track_id,X,artists,album_name,track_name,explicit,track_genre) )
```


```{r, results='markup'}
valence <- ggplot(data_corr, aes(x=valence)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth=.5,
                   colour="steelblue1", fill="white") +
    geom_density(alpha=.2, fill="steelblue1")

tempo <- ggplot(data_corr, aes(x=tempo)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth=.5,
                   colour="steelblue1", fill="white") +
    geom_density(alpha=.2, fill="steelblue1")

acousticness <- ggplot(data_corr, aes(x=acousticness)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth=.5,
                   colour="steelblue1", fill="white") +
    geom_density(alpha=.2, fill="steelblue1")

danceability <- ggplot(data_corr, aes(x=danceability)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth=.5,
                   colour="steelblue1", fill="white") +
    geom_density(alpha=.2, fill="steelblue1")

speechiness <- ggplot(data_corr, aes(x=speechiness)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth=.5,
                   colour="steelblue1", fill="white") +
    geom_density(alpha=.2, fill="steelblue1")

mode <- ggplot(data_corr, aes(x=mode)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth=.5,
                   colour="steelblue1", fill="white") +
    geom_density(alpha=.2, fill="steelblue1")

liveness <- ggplot(data_corr, aes(x=liveness)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth=.5,
                   colour="steelblue1", fill="white") +
    geom_density(alpha=.2, fill="steelblue1")

loudness <- ggplot(data_corr, aes(x=loudness)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth=.5,
                   colour="steelblue1", fill="white") +
    geom_density(alpha=.2, fill="steelblue1")

popularity <- ggplot(data_corr, aes(x=popularity)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth=.5,
                   colour="steelblue1", fill="white") +
    geom_density(alpha=.2, fill="steelblue1")

energy <- ggplot(data_corr, aes(x=energy)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth=.5,
                   colour="steelblue1", fill="white") +
    geom_density(alpha=.2, fill="steelblue1")


key <- ggplot(data_corr, aes(x=key)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth=.5,
                   colour="steelblue1", fill="white") +
    geom_density(alpha=.2, fill="steelblue1")

```


```{r, results='markup'}
library(ggpubr)
figure <- ggarrange(valence, tempo, acousticness,speechiness, danceability,mode,liveness,loudness,popularity,energy, key, ncol = 3, nrow = 5)
annotate_figure(figure, top = text_grob("Density Histogram for all variables", 
               color = "black", face = "bold", size = 14))
```

From the distribution plot, we can observe that, columns such as energy, and valence has high variability where as liveness, danceability, tempo and loudness has relatively lower variability and concentrated in some ranges.


```{r, results='markup'}
ggplot(data_corr, aes(x=energy, y=popularity)) + geom_point(col = "steelblue1") 
ggplot(data_corr, aes(x=acousticness, y=popularity)) + geom_point(col = "steelblue1") 
ggplot(data_corr, aes(x=loudness, y=popularity)) + geom_point(col = "steelblue1")
```

```{r, results='markup'}
data_corr <- data_corr %>% mutate(popularity_t = if_else(popularity > 50, 1, 0))
head(data_corr)
```
We have added a new column called popularity_t which is based on popularity feature to have a clear answer for our SMART questions. The top 25% of the popular songs will have 1 value and the rest will have 0.

```{r, results='markup'}
ggplot(data_corr, aes(x = popularity_t, fill=popularity_t)) +
    geom_bar(color="black",fill=c("steelblue3", "blue4"))+
  ggtitle("Popularity_t Distribution")
```
This bar plot represents the new column we have created, popularity_t. The top 25% of the popular songs have 1 value and the rest will have 0. We can observe most of the songs come under 0 category.

```{r, results='markup'}
#  t test on popularity_t with numeric variables
pop1 <- subset(data_corr, popularity_t == 1)
pop2 <- subset(data_corr, popularity_t == 0)
str(pop1)
str(pop2)

ttest_dance <- t.test(pop1$danceability, pop2$danceability)
ttest_dance
ttest_energy <- t.test(pop1$energy, pop2$energy)
ttest_energy
ttest_loudness <- t.test(pop1$loudness, pop2$loudness)
ttest_loudness
ttest_speechiness <- t.test(pop1$speechiness, pop2$speechiness)
ttest_speechiness
ttest_acousticness <- t.test(pop1$acousticness, pop2$acousticness)
ttest_acousticness
ttest_instrumentalness <- t.test(pop1$instrumentalness, pop2$instrumentalness)
ttest_instrumentalness
ttest_liveness <- t.test(pop1$liveness, pop2$liveness)
ttest_liveness
ttest_valence <- t.test(pop1$valence, pop2$valence)
ttest_valence
ttest_tempo <- t.test(pop1$tempo, pop2$tempo)
ttest_tempo
ttest_duration_ms <- t.test(pop1$duration_ms, pop2$duration_ms)
ttest_duration_ms

```

After created the popularity_t column, we split data into two based on the popularity_t. We would like to see the differences on means of numerical variables between two groups, so we decided to do hypothesis testing on the variables.
H0: The means of numerical variables will be same between different Popularity levels.
H1: The means of numerical variables will NOT be same between different Popularity levels.
For every numerical variables, means from the popular songs and less popular song were different since the p-values are small enough. Therefore we can conclude that numerical variables except for valence and energy affect the popularity. 

```{r boxplots for variables, results='markup'}
str(data_corr)
unique(data_corr$popularity_t)
poptfactor <- data_corr
poptfactor$popularity_t <- as.factor(data_corr$popularity_t)

boxt <- ggplot(poptfactor, aes(x=popularity_t, y=tempo,fill=popularity_t)) + 
  geom_boxplot() +
  ggtitle("boxplot for tempo")

boxdu <-ggplot(poptfactor, aes(x=popularity_t, y=duration_ms,fill=popularity_t)) + 
  geom_boxplot() +
  ggtitle("boxplot for duration_ms")

boxv <-ggplot(poptfactor, aes(x=popularity_t, y=valence,fill=popularity_t)) + 
  geom_boxplot() +
  ggtitle("boxplot for valence")

boxi <-ggplot(poptfactor, aes(x=popularity_t, y=instrumentalness,fill=popularity_t)) + 
  geom_boxplot() +
  ggtitle("boxplot for instrumentalness")

boxa <-ggplot(poptfactor, aes(x=popularity_t, y=acousticness,fill=popularity_t)) + 
  geom_boxplot() +
  ggtitle("boxplot for acousticness")

boxda <-ggplot(poptfactor, aes(x=popularity_t, y=danceability,fill=popularity_t)) + 
  geom_boxplot() +
  ggtitle("boxplot for danceability")

boxe <-ggplot(poptfactor, aes(x=popularity_t, y=energy,fill=popularity_t)) + 
  geom_boxplot() +
  ggtitle("boxplot for energy")

boxlo <-ggplot(poptfactor, aes(x=popularity_t, y=loudness,fill=popularity_t)) + 
  geom_boxplot() +
  ggtitle("boxplot for loudness")

boxv <-ggplot(poptfactor, aes(x=popularity_t, y=valence,fill=popularity_t)) + 
  geom_boxplot(col="black", fill=c("steelblue1","blue4")) +
  ggtitle("boxplot for valence")

boxi <-ggplot(poptfactor, aes(x=popularity_t, y=instrumentalness,fill=popularity_t)) + 
  geom_boxplot(col="black", fill=c("steelblue1","blue4")) +
  ggtitle("boxplot for instrumentalness")

boxa <-ggplot(poptfactor, aes(x=popularity_t, y=acousticness,fill=popularity_t)) + 
  geom_boxplot(col="black", fill=c("steelblue1","blue4")) +
  ggtitle("boxplot for acousticness")

boxda <-ggplot(poptfactor, aes(x=popularity_t, y=danceability,fill=popularity_t)) + 
  geom_boxplot(col="black", fill=c("steelblue1","blue4")) +
  ggtitle("boxplot for danceability")

boxe <-ggplot(poptfactor, aes(x=popularity_t, y=energy,fill=popularity_t)) + 
  geom_boxplot(col="black", fill=c("steelblue1","blue4")) +
  ggtitle("boxplot for energy")

boxlo <-ggplot(poptfactor, aes(x=popularity_t, y=loudness,fill=popularity_t)) + 
  geom_boxplot(col="black", fill=c("steelblue1","blue4")) +
  ggtitle("boxplot for loudness")

boxs <-ggplot(poptfactor, aes(x=popularity_t, y=speechiness,fill=popularity_t)) + 
  geom_boxplot(col="black", fill=c("steelblue1","blue4")) +
  ggtitle("boxplot for speechiness")

boxli <-ggplot(poptfactor, aes(x=popularity_t, y=liveness,fill=popularity_t)) + 
  geom_boxplot(col="black", fill=c("steelblue1","blue4")) +
  ggtitle("boxplot for liveness")

```

These are boxplots for each numerical variable that separated by its popularity level. 

```{r, results='markup'}
library("ggpubr")
boxforvar <- ggarrange(boxt, boxv,boxi,boxa,boxda,boxe,boxlo,boxs, boxli, ncol = 3, nrow = 4)
boxforvar
```

```{r, results='hide'}
boxt
boxv
boxi
boxa
boxda
boxe
boxlo
boxs
boxli
# boxplots
```

```{r, results='markup'}

boxs <-ggplot(poptfactor, aes(x=popularity_t, y=speechiness,fill=popularity_t)) + 
  geom_boxplot() +
  ggtitle("boxplot for speechiness")

boxli <-ggplot(poptfactor, aes(x=popularity_t, y=liveness,fill=popularity_t)) + 
  geom_boxplot() +
  ggtitle("boxplot for liveness")

```

```{r, results='markup'}
boxforvar <- ggarrange(boxt, boxdu, boxv,boxi,boxa,boxda,boxe,boxlo,boxs, boxli, ncol = 3, nrow = 4)
boxforvar
```

```{r chisq on pop_t and categorical, results='markup'}
chi_key <- chisq.test(table(data_corr$popularity_t, data_corr$key))
chi_key
chi_mode <- chisq.test(table(data_corr$popularity_t, data_corr$mode))
chi_mode
chi_time_signature <- chisq.test(table(data_corr$popularity_t, data_corr$time_signature))
chi_time_signature
```
Since we have popularity t which is categorical value, we performed the chi-square test with categorical variables which is key, mode, and time signature.  
The null hypothesis here are will be whether the song is popular or not is independent with key/mode/time signature.
The alternate hypothesis are they are NOT independent.
From the tests, the p-values are small enough to reject the null, which means that they are dependent.


```{r t test and ANOVA on popularity by categorical variables,  results='markup'}
unique(pop1$key)
unique(pop1$mode)
unique(pop1$time_signature)
mode1 <- subset(data_corr, mode == 1)
mode0 <- subset(data_corr, mode == 0)
str(mode1)
str(mode0)

ttest_mode <- t.test(mode1$popularity, mode0$popularity)
ttest_mode

aovtime_signature <- aov(popularity ~ time_signature, data = data_corr)
summary(aovtime_signature)
```
Also, these are results of statistical test that performed with the original popularity and categorical variables such as mode and time signature to see the differences between means from different categories. 
For mode, since it has two value, we performed the t-test. The null hypothesis here is the means from different modes have same values. The alternate hypothesis is they have different values by modes.
For time signature, we took ANOVA test since it has 5 different categories. The null hypothesis here is the means from different time signature have same values. The alternate hypothesis is they have different values by time signature.
For both, p-value is small enough to reject the null hypothesis.
 





# 6. Model Building

## 6.1 Feature Selection

We made an effort to decide which features should be present before developing a model. As a result, we tried to see the Mallow's Cp statistic and performed feature selection. In contrast, when we chose our features based on the lowest Cp value, we obtained an excessively high accuracy score, indicating that the model was over-fitted. Therefore, we made the decision to construct a complete model with all the features.

```{r, results='markup'}
library("leaps")
reg2.best <- regsubsets(popularity_t~. , data = data_corr, nvmax = 12, nbest = 3, method = "exhaustive") 
plot(reg2.best, scale = "Cp", main = "Cp")
```


## 6.2 Spliting the data


To Prepare your data for training We are Splitting the data into train and testing dataset.We are Separating features and target label.Our target label is popularity_t and features are all other parameters available in the dataset. 

```{r}
set.seed(123)
dat.d <- sample(1:nrow(data_corr),size=nrow(data_corr)*0.8,replace = FALSE) #random selection of 70% data.
data_corr.a = subset(data_corr, select = c(-popularity,-duration_ms) )
 
 
train <- data_corr.a[dat.d,] # 80% training data
test <- data_corr.a[-dat.d,] # remaining 20% test data
```



## 6.3 Which Model to use?

This is a classification problem where we need to detect if song will be popular(1) or not popular (0). There are multiple algorithms for classification problems We are going to apply the below algorithms.

1. KNN Algorithm
2. Logistic Regression
3. Decision Trees

To answer our SMART question, "Can we recommend certain songs to users who like to listen popular songs?", we are using above classfication models to classify the popular and unpopular songs based on the features that we analyzed. The model with the highest accuracy will be adopted. 

## 6.4 KNN
The first model we tried to build is KNN.

```{r}
library(class)
prc_train_labels <- train[,14]
prc_test_labels<- test[,14]
```



### 6.4.1. Selecting the correct "k"
How does "k" affect classification accuracy? Let's create a function to calculate classification accuracy based on the number of "k."
```{r, results='markup'}
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k) #,                #<- number of neighbors considered
                  # use.all = TRUE)       #<- control ties between class assignments. If true, all distances equal to the k-th largest are included
  
  tab = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(tab[row(tab) == col(tab)]) / sum(tab)                         
  cbind(k = k, accuracy = accu)
}

# The sapply() function plugs in several values into our chooseK function.
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop.
knn_different_k = sapply(seq(3, 15, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = train,
                                             val_set = test,
                                             train_class = prc_train_labels,
                                             val_class = prc_test_labels))

# Reformat the results to graph the results.
str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg("ggplot2")

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "steelblue1", size = 1.5) +
  geom_point(size = 3) + 
  labs(title = "accuracy vs k")

```

From the above results we can observe that for value k=3, we are getting the maximum accuracy. We shall go forward and evaluate the model performance by plotting the confusion matrix of the model.

```{r}
prc_test_pred <- knn(train = train, test = test,cl = prc_train_labels, k=3)
```

###6.4.2 Confusion Matrix

```{r, results='markup'}
cm_knn<-confusionMatrix(as.factor(prc_test_labels),as.factor(prc_test_pred))
cm_knn
```

This model's overall accuracy, which compares favorably to the other two models, is 92.55%. Here, we can see that 16757 songs were correctly projected as unpopular, while 4345 songs were successfully forecasted as popular. In contrast, 478 songs were incorrectly predicted as unpopular, while 1220 songs were incorrectly predicted as popular.

## 6.5 Logistic Regression Model

Next, we are building the logistic regression model. 

```{r}
model = glm(popularity_t~.,data=train )
```

### 6.5.1 Confusion Matrix

```{r, results='markup'}

Predicr_value = predict(model,test)                          #predict test value purchased

predict_test_value = ifelse((Predicr_value>0.5),1,0)

                           #See predict value and actual
Compare_values = cbind(predict_test_value,test[,14])
Compare_values = data.frame(Compare_values)
cm_lg<-caret::confusionMatrix(as.factor(Compare_values[,1]),as.factor(Compare_values[,2]))
cm_lg
```

As you can see, there are certain instances where disliked music have been mistakenly categorized as popular. The classification prediction accuracy is good, coming in at roughly 75.59%. 24.4% of classification errors are misclassified. Here, the summary shows that, with the exception of the key variable, all other variables have a significant impact on popularity.

## 6.6 Decision Tree

Lastly, we tried to build a decision tree model. 

```{r}
library(rpart)
library(rpart.plot)

fit <- rpart(popularity_t~., data = train, method = 'class')
```

### 6.6.1 Confusion Matrix
```{r}
predict_unseen <-predict(fit, test, type = 'class')
table_mat <- table(test$popularity_t, predict_unseen)
```

```{r, results='markup'}
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
print(paste('Accuracy for test', accuracy_Test))
cm_dt<-confusionMatrix(as.factor(test$popularity_t),as.factor(predict_unseen))
cm_dt
```

As you can see, there are several instances where songs that are considered to be popular but are actually considered unpopular. For a straightforward decision tree classifier, the model's overall accuracy of 75.59%, which is comparable to logistic regression, is more than acceptable.

## Comparing all the models

```{r, results='markup'}


model_compare <- data.frame(Model = c('KNN',
                                      'Decision Tree',
                                      'Logistic Regression'),
                            Accuracy = c((cm_knn$overall[1])*100,
                                         (cm_dt$overall[1])*100,
                                         (cm_lg$overall[1])*100))

print(model_compare)

ggplot(aes(x=Model, y=Accuracy), data=model_compare) +
    geom_bar(stat='identity', fill = 'steelblue1') +
    ggtitle('Comparative Accuracy of Models on Cross-Validation Data') +
    xlab('Models') +
    ylab('Overall Accuracy')
```

Comparing all the three models we can conclude that KNN is the best algorithm to classify popular and unpopular songs having 92.55% accuracy. Therefore, it would be better to use KNN to answer our SMART question for the conclusion. 

# 5. Conclusion

The main question we are trying to answer with all the models is "Can we recommend certain songs to users who like to listen popular songs?". According to our analysis, the answer will be "YES". By using KNN model that we built, we can classify popular and unpopular song. Thus we can use this prediction model to recommend a certain song to users on Spotify who like to listen only popular songs.  
Through the exploratory data analysis, we can identify audio features which affect the popularity of songs to answer the first question. All the numerical variables such as danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, and tempo affect the popularity of songs since they have relationship with popularity statistically. For categorical variables, such as key, mode, and time signature, they are also dependent to popularity from the results of chi-squared tests.  
In a future project, we would like to include other features that we couldn't explore this time like artist name or genre that heavily influence the popularity of a song in our analysis to acquire different insights.

